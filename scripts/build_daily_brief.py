import os
import re
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil import tz
import time

# =========================
# 1. ê¸°ë³¸ ì„¤ì •
# =========================
KST = tz.gettz("Asia/Seoul")
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
POSTS_DIR = os.path.join(ROOT, "posts")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# ëª¨ë¸ëª…ì€ ê°€ì¥ íš¨ìœ¨ì ì¸ gpt-4o-minië¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
OPENAI_MODEL = os.getenv("OPENAI_MODEL") or "gpt-4o-mini"

SOURCES = [
    {"id": "sbs_headline", "name": "SBS (ì´ ì‹œê° ì´ìŠˆ)", "url": "https://news.sbs.co.kr/news/headlineRssFeed.do?plink=RSSREADER", "limit": 5},
    {"id": "sbs_politics", "name": "SBS (ì •ì¹˜)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER", "limit": 5},
    {"id": "yonhap_tv_latest", "name": "ì—°í•©ë‰´ìŠ¤TV (ìµœì‹ )", "url": "http://www.yonhapnewstv.co.kr/browse/feed/", "limit": 5},
    {"id": "mk_economy", "name": "ë§¤ì¼ê²½ì œ (ê²½ì œ)", "url": "https://www.mk.co.kr/rss/30100041/", "limit": 5},
    {"id": "hankyung_economy", "name": "í•œêµ­ê²½ì œ (ê²½ì œ)", "url": "https://www.hankyung.com/feed/economy", "limit": 5},
    {"id": "koreakr_policy", "name": "ì •ì±…ë¸Œë¦¬í•‘ (ì •ì±…ë‰´ìŠ¤)", "url": "https://www.korea.kr/rss/policy.xml", "limit": 8},
]

# =========================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================
def ensure_dir():
    os.makedirs(POSTS_DIR, exist_ok=True)

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def fetch_full_content(url: str) -> str:
    """ê¸°ì‚¬ ì›ë¬¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (í¬ë¡¤ë§ ê°•í™”)"""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for s in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form']):
            s.decompose()
            
        # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­ íƒœê·¸ ì°¾ê¸°
        content = soup.find('article') or soup.find('div', id='articleBody') or soup.find('div', class_='article_view') or soup.find('div', id='news_body_area')
        
        if content:
            return clean_text(content.get_text())
        return ""
    except:
        return ""

def openai_summary(title: str, content: str) -> str | None:
    """OpenAI APIë¥¼ ì´ìš©í•œ 5ë¬¸ì¥ ìš”ì•½ (í‘œì¤€ API ë°©ì‹)"""
    if not OPENAI_API_KEY:
        return None

    input_text = content if len(content) > 150 else title
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {"role": "system", "content": "ë„ˆëŠ” í•œêµ­ ë‰´ìŠ¤ ì „ë¬¸ ì—ë””í„°ë‹¤. ë‚´ìš©ì„ [í•µì‹¬ ì‚¬ì‹¤], [ë°°ê²½], [ì˜í–¥ ë° ì „ë§]ì´ í¬í•¨ë˜ë„ë¡ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ 5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ë¼."},
            {"role": "user", "content": f"ì œëª©: {title}\n\në³¸ë¬¸: {input_text[:3500]}"}
        ],
        "temperature": 0.5,
    }

    try:
        r = requests.post(url, headers=headers, json=payload, timeout=40)
        r.raise_for_status()
        return r.json()['choices'][0]['message']['content'].strip()
    except Exception as e:
        print(f"API Error: {e}")
        return None

def generate_markdown(items):
    """ê°€ë…ì„± ë†’ì€ README ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
    now = datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M:%S")
    md = f"# ğŸ“° ë§¤ì¼ ë‰´ìŠ¤ ìš”ì•½ ì„œë¹„ìŠ¤\n\n"
    md += f"> **ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** {now} (KST)\n\n"
    md += "ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ë¥¼ AIê°€ ë¶„ì„í•˜ì—¬ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤. **ì œëª©ì„ í´ë¦­**í•˜ë©´ ìƒì„¸ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
    
    icons = {"politics": "âš–ï¸", "economy": "ğŸ’°", "headline": "ğŸ”¥", "policy": "ğŸ“¢", "default": "ğŸ“Œ"}

    for item in items:
        cat_key = next((k for k in icons if k in item['category']), "default")
        icon = icons[cat_key]
        
        md += f"### {icon} {item['title']}\n"
        md += f"<details>\n<summary>ğŸ” ìš”ì•½ ë³´ê¸° (ì¶œì²˜: {item['source']})</summary>\n\n"
        md += f"**AI ìš”ì•½:**\n\n{item['summary']}\n\n"
        md += f"[ğŸ”— ê¸°ì‚¬ ì›ë¬¸ ë§í¬]({item['url']})\n"
        md += f"</details>\n\n---\n"
    
    md += "\n\n---\n*ë³¸ ì½˜í…ì¸ ëŠ” OpenAI GPTë¥¼ í†µí•´ ìë™ ìš”ì•½ë˜ì—ˆìŠµë‹ˆë‹¤.*"
    return md

# =========================
# 3. ë©”ì¸ ì‹¤í–‰ë¶€
# =========================
def main():
    ensure_dir()
    collected_items = []
    seen_titles = set()

    for s in SOURCES:
        print(f"ìˆ˜ì§‘ ì¤‘: {s['name']}...")
        feed = feedparser.parse(s["url"])
        
        count = 0
        for e in feed.entries:
            if count >= s["limit"]: break
            
            title = e.get("title", "").strip()
            link = e.get("link", "").strip()
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ì œëª© ì• 12ì ë¹„êµ)
            title_key = title[:12].replace(" ", "")
            if title_key in seen_titles: continue
            seen_titles.add(title_key)

            # ë³¸ë¬¸ ì¶”ì¶œ ë° ìš”ì•½
            full_text = fetch_full_content(link)
            if not full_text:
                full_text = clean_text(e.get("summary", ""))

            summary = openai_summary(title, full_text)
            if not summary:
                summary = (full_text[:200] + "...") if full_text else "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            collected_items.append({
                "category": s["id"],
                "source": s["name"],
                "title": title,
                "url": link,
                "summary": summary
            })
            count += 1
            time.sleep(0.5)

    # ê²°ê³¼ ì €ì¥ (JSON)
    today = datetime.now(tz=KST).strftime("%Y-%m-%d")
    data = {"generated_at": datetime.now(tz=KST).isoformat(), "items": collected_items}
    
    with open(os.path.join(POSTS_DIR, "latest.json"), "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    with open(os.path.join(POSTS_DIR, f"{today}.json"), "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # README.md ì—…ë°ì´íŠ¸
    readme_content = generate_markdown(collected_items)
    with open(os.path.join(ROOT, "README.md"), "w", encoding="utf-8") as f:
        f.write(readme_content)

    print(f"ì™„ë£Œ: {len(collected_items)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
