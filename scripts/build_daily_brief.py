import os
import re
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil import tz
import time

# =========================
# 1. ê¸°ë³¸ ì„¤ì •
# =========================
KST = tz.gettz("Asia/Seoul")
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
POSTS_DIR = os.path.join(ROOT, "posts")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL") or "gpt-4o-mini"

# ì¥ë¥´ë³„ ë‰´ìŠ¤ ì†ŒìŠ¤ ì„¤ì •
SOURCES = [
    {"id": "politics", "name": "ì •ì¹˜ (SBS)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER", "limit": 5},
    {"id": "politics", "name": "ì •ì¹˜ (ë§¤ê²½)", "url": "https://www.mk.co.kr/rss/30200030/", "limit": 5},
    {"id": "economy", "name": "ê²½ì œ (SBS)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02&plink=RSSREADER", "limit": 5},
    {"id": "economy", "name": "ê²½ì œ (í•œê²½)", "url": "https://www.hankyung.com/feed/economy", "limit": 5},
    {"id": "headline", "name": "ì£¼ìš”ë‰´ìŠ¤ (ì—°í•©TV)", "url": "http://www.yonhapnewstv.co.kr/browse/feed/", "limit": 5},
    {"id": "policy", "name": "ì •ì±…ë¸Œë¦¬í•‘", "url": "https://www.korea.kr/rss/policy.xml", "limit": 10},
]

# HTML ìƒë‹¨ì— í‘œì‹œë  ì¹´í…Œê³ ë¦¬ ë²„íŠ¼ ì •ì˜
DISPLAY_CATEGORIES = [
    {"id": "all", "name": "ì „ì²´"},
    {"id": "headline", "name": "ğŸ”¥ ì£¼ìš”ì†Œì‹"},
    {"id": "politics", "name": "âš–ï¸ ì •ì¹˜"},
    {"id": "economy", "name": "ğŸ’° ê²½ì œ/IT"},
    {"id": "policy", "name": "ğŸ“¢ ì •ë¶€/ì •ì±…"}
]

# =========================
# 2. ìœ í‹¸ë¦¬í‹° ë° í¬ë¡¤ë§
# =========================
def ensure_dir():
    os.makedirs(POSTS_DIR, exist_ok=True)

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def fetch_full_content(url: str) -> str:
    """ê¸°ì‚¬ ì›ë¬¸ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        for s in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form']):
            s.decompose()
        content = soup.find('article') or soup.find('div', id='articleBody') or soup.find('div', class_='article_view') or soup.find('div', id='news_body_area')
        return clean_text(content.get_text()) if content else ""
    except:
        return ""

def openai_summary(title: str, content: str) -> str | None:
    """OpenAI API ìš”ì•½"""
    if not OPENAI_API_KEY: return None
    input_text = content if len(content) > 150 else title
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {
                "role": "system", 
                "content": (
                    "ë„ˆëŠ” ì „ë¬¸ì ì¸ ë‰´ìŠ¤ ë¶„ì„ê°€ì´ì ì—ë””í„°ë‹¤. "
                    "ë…ìê°€ ì›ë¬¸ì„ ë³´ì§€ ì•Šì•„ë„ ë§¥ë½ì„ ì™„ë²½íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ 'ì‹¬ì¸µ ë¶„ì„ ìš”ì•½'ì„ ì œê³µí•œë‹¤. "
                    "ë‚´ìš©ì€ ë°˜ë“œì‹œ ë‹¤ìŒ 3ê°€ì§€ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´ 10ë¬¸ì¥ ë‚´ì™¸ë¡œ ì‘ì„±í•˜ë¼.\n\n"
                    "1. [ì‚¬ê±´ì˜ í•µì‹¬]: ëˆ„ê°€, ì–¸ì œ, ë¬´ì—‡ì„ í–ˆëŠ”ì§€ ìƒì„¸íˆ ê¸°ìˆ  (3-4ë¬¸ì¥)\n"
                    "2. [ë§¥ë½ê³¼ ë°°ê²½]: ì´ ì‚¬ê±´ì´ ì™œ ë°œìƒí–ˆëŠ”ì§€, ì´ì „ ìƒí™©ì€ ì–´ë– í–ˆëŠ”ì§€ ì„¤ëª… (3ë¬¸ì¥)\n"
                    "3. [ìŸì  ë° ì „ë§]: ì•ìœ¼ë¡œì˜ ì˜í–¥, ì´í•´ê´€ê³„ìë“¤ì˜ ì…ì¥, í–¥í›„ ê´€ì „ í¬ì¸íŠ¸ ì œì‹œ (3ë¬¸ì¥)\n\n"
                    "ê²©ì‹ ìˆê³  ë¶„ì„ì ì¸ í†¤ì„ ìœ ì§€í•˜ë©°, êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê³ ìœ ëª…ì‚¬ê°€ ë³¸ë¬¸ì— ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ë¼."
                )
            },
            {
                "role": "user", 
                "content": f"ì œëª©: {title}\n\në³¸ë¬¸: {input_text[:3500]}"
            }
        ],
        "temperature": 0.5,
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=40)
        return r.json()['choices'][0]['message']['content'].strip()
    except:
        return None

def generate_markdown(items):
    """READMEìš© ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
    now = datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M:%S")
    md = f"# ğŸ“° World Brief ë‰´ìŠ¤ ìš”ì•½\n\n> ì—…ë°ì´íŠ¸: {now}\n\n"
    for item in items[:15]: # ìƒìœ„ 15ê°œë§Œ ìš”ì•½ ë…¸ì¶œ
        md += f"### {item['title']}\n<details><summary>ìš”ì•½ ë³´ê¸° ({item['source']})</summary>\n\n{item['summary']}\n\n[ì›ë¬¸ ì½ê¸°]({item['url']})\n</details>\n\n"
    return md

# =========================
# 3. ë©”ì¸ ì‹¤í–‰
# =========================
def main():
    ensure_dir()
    collected_items = []
    seen_titles = set()

    for s in SOURCES:
        print(f"ìˆ˜ì§‘ ì¤‘: {s['name']}...")
        feed = feedparser.parse(s["url"])
        
        count = 0
        for e in feed.entries:
            if count >= s["limit"]: break
            title = e.get("title", "").strip()
            link = e.get("link", "").strip()
            
            # ì¤‘ë³µ ì²´í¬
            title_key = title[:12].replace(" ", "")
            if title_key in seen_titles: continue
            seen_titles.add(title_key)

            full_text = fetch_full_content(link) or clean_text(e.get("summary", ""))
            summary = openai_summary(title, full_text) or (full_text[:200] + "...")

            collected_items.append({
                "category": s["id"], # politics, economy ë“±
                "source": s["name"],
                "title": title,
                "url": link,
                "published_at": datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M"),
                "summary": summary
            })
            count += 1
            time.sleep(0.3)

    # ë°ì´í„° êµ¬ì„±
    final_data = {
        "generated_at": datetime.now(tz=KST).isoformat(),
        "categories": DISPLAY_CATEGORIES,
        "items": collected_items
    }

    # íŒŒì¼ ì €ì¥
    today = datetime.now(tz=KST).strftime("%Y-%m-%d")
    for f_path in ["latest.json", f"{today}.json"]:
        with open(os.path.join(POSTS_DIR, f_path), "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

    # README ì—…ë°ì´íŠ¸
    with open(os.path.join(ROOT, "README.md"), "w", encoding="utf-8") as f:
        f.write(generate_markdown(collected_items))

    print(f"ì™„ë£Œ: {len(collected_items)}ê°œ í•­ëª© ì €ì¥.")

if __name__ == "__main__":
    main()
