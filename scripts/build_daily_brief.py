import os
import re
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil import tz
import time

# =========================
# 1. ê¸°ë³¸ ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜
# =========================
KST = tz.gettz("Asia/Seoul")
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
POSTS_DIR = os.path.join(ROOT, "posts")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL") or "gpt-4o-mini"

# ìˆ˜ì§‘ëŸ‰ì„ ëŠ˜ë¦¬ê¸° ìœ„í•´ limitì„ ìƒí–¥ ì¡°ì •í–ˆìŠµë‹ˆë‹¤. (ì´ í•©ê³„ ì•½ 75ê°œ -> ì¤‘ë³µ ì œê±° í›„ 50~60ê°œ ëª©í‘œ)
SOURCES = [
    {"id": "politics", "name": "ì •ì¹˜ (SBS)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER", "limit": 12},
    {"id": "politics", "name": "ì •ì¹˜ (ë§¤ê²½)", "url": "https://www.mk.co.kr/rss/30200030/", "limit": 12},
    {"id": "economy", "name": "ê²½ì œ (SBS)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02&plink=RSSREADER", "limit": 12},
    {"id": "economy", "name": "ê²½ì œ (í•œê²½)", "url": "https://www.hankyung.com/feed/economy", "limit": 12},
    {"id": "headline", "name": "ì£¼ìš”ë‰´ìŠ¤ (ì—°í•©TV)", "url": "http://www.yonhapnewstv.co.kr/browse/feed/", "limit": 12},
    {"id": "policy", "name": "ì •ì±…ë¸Œë¦¬í•‘", "url": "https://www.korea.kr/rss/policy.xml", "limit": 15},
]

DISPLAY_CATEGORIES = [
    {"id": "all", "name": "ì „ì²´"},
    {"id": "headline", "name": "ğŸ”¥ ì£¼ìš”ì†Œì‹"},
    {"id": "politics", "name": "âš–ï¸ ì •ì¹˜"},
    {"id": "economy", "name": "ğŸ’° ê²½ì œ/IT"},
    {"id": "policy", "name": "ğŸ“¢ ì •ë¶€/ì •ì±…"}
]

# =========================
# 2. í¬ë¡¤ë§ ë° ë¶„ì„ ìœ í‹¸ë¦¬í‹°
# =========================
def ensure_dir():
    os.makedirs(POSTS_DIR, exist_ok=True)

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def fetch_full_content(url: str) -> str:
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        for s in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'iframe']):
            s.decompose()
        content = soup.find('article') or soup.find('div', id='articleBody') or soup.find('div', class_='article_view') or soup.find('div', id='news_body_area')
        return clean_text(content.get_text()) if content else ""
    except:
        return ""

def openai_summary(title: str, content: str) -> str | None:
    if not OPENAI_API_KEY: return None
    
    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œëª© í™œìš©, ê¸¸ë©´ 3500ìê¹Œì§€ ì‚¬ìš©
    input_text = content if len(content) > 150 else title
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    
    # ì›ë¬¸ì„ ì•ˆ ë´ë„ ë  ì •ë„ë¡œ ì•Œì°¬ 10ë¬¸ì¥ ì‹¬ì¸µ ìš”ì•½ í”„ë¡¬í”„íŠ¸
    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {
                "role": "system", 
                "content": (
                    "ë„ˆëŠ” ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ê°€ë‹¤. ë…ìê°€ ì›ë¬¸ì„ ë³´ì§€ ì•Šì•„ë„ ë§¥ë½ì„ ì™„ë²½íˆ ì´í•´í•˜ë„ë¡ ì‹¬ì¸µ ìš”ì•½ì„ ì œê³µí•œë‹¤. "
                    "ë‚´ìš©ì€ ë°˜ë“œì‹œ ë‹¤ìŒ 3ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´ 10ë¬¸ì¥ ë‚´ì™¸ë¡œ ì‘ì„±í•˜ë¼.\n\n"
                    "1. [í•µì‹¬ ì‚¬ì‹¤]: ì‚¬ê±´ì˜ í•µì‹¬ ìš”ì§€ë¥¼ ìƒì„¸íˆ ê¸°ìˆ  (3-4ë¬¸ì¥)\n"
                    "2. [ë§¥ë½ê³¼ ë°°ê²½]: ì´ ì‚¬ê±´ì´ ì™œ ë°œìƒí–ˆëŠ”ì§€, ì´ì „ ìƒí™©ì€ ì–´ë– í–ˆëŠ”ì§€ ì„¤ëª… (3ë¬¸ì¥)\n"
                    "3. [ì „ë§ ë° ë¶„ì„]: ì•ìœ¼ë¡œì˜ ì˜í–¥ê³¼ í–¥í›„ ê´€ì „ í¬ì¸íŠ¸ ì œì‹œ (3ë¬¸ì¥)\n\n"
                    "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê³ ìœ ëª…ì‚¬ë¥¼ í¬í•¨í•˜ì—¬ ë¶„ì„ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
                )
            },
            {"role": "user", "content": f"ì œëª©: {title}\n\në³¸ë¬¸: {input_text[:3500]}"}
        ],
        "temperature": 0.5,
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=45)
        return r.json()['choices'][0]['message']['content'].strip()
    except:
        return None

def generate_markdown(items):
    now = datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M:%S")
    md = f"# ğŸ“° World Brief ì‹¬ì¸µ ë‰´ìŠ¤ ìš”ì•½\n\n> **ì—…ë°ì´íŠ¸:** {now} (KST)\n\n"
    md += "ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì„¹ì…˜ë³„ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ì œëª©ì„ í´ë¦­í•´ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.\n\n"
    
    for item in items[:20]: # READMEì—ëŠ” ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
        md += f"### {item['title']}\n<details><summary>ğŸ” ì‹¬ì¸µ ë¶„ì„ ë³´ê¸° (ì¶œì²˜: {item['source']})</summary>\n\n{item['summary']}\n\n[ğŸ”— ì›ë¬¸ ë§í¬]({item['url']})\n</details>\n\n---\n"
    return md

# =========================
# 3. ë©”ì¸ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤
# =========================
def main():
    ensure_dir()
    collected_items = []
    seen_titles = set()

    for s in SOURCES:
        print(f"ğŸ“¡ ìˆ˜ì§‘ ì¤‘: {s['name']} (ìµœëŒ€ {s['limit']}ê°œ)...")
        feed = feedparser.parse(s["url"])
        
        count = 0
        for e in feed.entries:
            if count >= s["limit"]: break
            
            title = e.get("title", "").strip()
            link = e.get("link", "").strip()
            
            # ì œëª© ì• 12ê¸€ì ê¸°ë°˜ ì§€ëŠ¥í˜• ì¤‘ë³µ ì œê±°
            title_key = title[:12].replace(" ", "")
            if title_key in seen_titles: continue
            seen_titles.add(title_key)

            # ë³¸ë¬¸ ì¶”ì¶œ ë° ì‹¬ì¸µ ìš”ì•½
            full_text = fetch_full_content(link) or clean_text(e.get("summary", ""))
            summary = openai_summary(title, full_text)
            
            if not summary:
                summary = "ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì›ë¬¸ì„ ì°¸ê³ í•´ ì£¼ì„¸ìš”."

            collected_items.append({
                "category": s["id"],
                "source": s["name"],
                "title": title,
                "url": link,
                "published_at": datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M"),
                "summary": summary
            })
            count += 1
            # API ê³¼ë¶€í•˜ ë°©ì§€ ë° ì•ˆì •ì  ìˆ˜ì§‘ì„ ìœ„í•œ ì§€ì—°
            time.sleep(0.5)

    # ìµœì¢… ë°ì´í„° êµ¬ì„± (ìµœëŒ€ 60ê°œë¡œ ì œí•œ)
    final_data = {
        "generated_at": datetime.now(tz=KST).isoformat(),
        "categories": DISPLAY_CATEGORIES,
        "items": collected_items[:60]
    }

    # JSON íŒŒì¼ ì €ì¥
    today = datetime.now(tz=KST).strftime("%Y-%m-%d")
    for filename in ["latest.json", f"{today}.json"]:
        with open(os.path.join(POSTS_DIR, filename), "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

    # README.md ìë™ ì—…ë°ì´íŠ¸
    readme_content = generate_markdown(collected_items)
    with open(os.path.join(ROOT, "README.md"), "w", encoding="utf-8") as f:
        f.write(readme_content)

    print(f"âœ… ì™„ë£Œ: ì´ {len(final_data['items'])}ê°œì˜ ì‹¬ì¸µ ë‰´ìŠ¤ ìš”ì•½ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
