import os
import re
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil import tz
import time

# =========================
# 1. ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜
# =========================
KST = tz.gettz("Asia/Seoul")
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
POSTS_DIR = os.path.join(ROOT, "posts")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL") or "gpt-4o-mini"

# ì‚¬íšŒ, ê³¼í•™ì„ í¬í•¨í•œ ë‰´ìŠ¤ ì†ŒìŠ¤ (ì „ì²´ ì•½ 60~70ê°œ ìˆ˜ì§‘ -> ì¤‘ë³µ ì œê±° í›„ 50~60ê°œ ëª©í‘œ)
SOURCES = [
    {"id": "headline", "name": "ì£¼ìš”ë‰´ìŠ¤ (ì—°í•©TV)", "url": "http://www.yonhapnewstv.co.kr/browse/feed/", "limit": 10},
    {"id": "society", "name": "ì‚¬íšŒ (YTN)", "url": "https://www.ytn.co.kr/_ln/rss/0103.xml", "limit": 10},
    {"id": "politics", "name": "ì •ì¹˜ (SBS)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER", "limit": 10},
    {"id": "economy", "name": "ê²½ì œ (í•œê²½)", "url": "https://www.hankyung.com/feed/economy", "limit": 10},
    {"id": "science", "name": "IT/ê³¼í•™ (SBS)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=08&plink=RSSREADER", "limit": 10},
    {"id": "science", "name": "ê³¼í•™ (ë§¤ê²½)", "url": "https://www.mk.co.kr/rss/30100041/", "limit": 10},
    {"id": "policy", "name": "ì •ì±…ë¸Œë¦¬í•‘", "url": "https://www.korea.kr/rss/policy.xml", "limit": 12},
]

DISPLAY_CATEGORIES = [
    {"id": "all", "name": "ì „ì²´"},
    {"id": "headline", "name": "ğŸ”¥ ì£¼ìš”ì†Œì‹"},
    {"id": "politics", "name": "âš–ï¸ ì •ì¹˜"},
    {"id": "economy", "name": "ğŸ’° ê²½ì œ/IT"},
    {"id": "society", "name": "ğŸ‘¥ ì‚¬íšŒ/ìƒí™œ"},
    {"id": "science", "name": "ğŸ§ª ê³¼í•™/ê¸°ìˆ "},
    {"id": "policy", "name": "ğŸ“¢ ì •ë¶€/ì •ì±…"}
]

# =========================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================
def ensure_dir():
    os.makedirs(POSTS_DIR, exist_ok=True)

def clean_text(text: str) -> str:
    if not text: return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def fetch_full_content(url: str) -> str:
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        for s in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'iframe']):
            s.decompose()
        content = soup.find('article') or soup.find('div', id='articleBody') or soup.find('div', class_='article_view') or soup.find('div', id='news_body_area')
        return clean_text(content.get_text()) if content else ""
    except:
        return ""

def openai_summary(title: str, content: str) -> str | None:
    if not OPENAI_API_KEY: return None
    input_text = content if len(content) > 150 else title
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    
    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {
                "role": "system", 
                "content": (
                    "ë„ˆëŠ” ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ê°€ë‹¤. ë…ìê°€ ì›ë¬¸ì„ ë³´ì§€ ì•Šì•„ë„ ë§¥ë½ì„ ì™„ë²½íˆ ì´í•´í•˜ë„ë¡ ì‹¬ì¸µ ìš”ì•½ì„ ì œê³µí•œë‹¤. "
                    "ë‚´ìš©ì€ ë°˜ë“œì‹œ ë‹¤ìŒ 3ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´ 10ë¬¸ì¥ ë‚´ì™¸ë¡œ ì‘ì„±í•˜ë¼.\n\n"
                    "1. [í•µì‹¬ ì‚¬ì‹¤]: ì‚¬ê±´ì˜ í•µì‹¬ ìš”ì§€ë¥¼ ìƒì„¸íˆ ê¸°ìˆ  (3-4ë¬¸ì¥)\n"
                    "2. [ë§¥ë½ê³¼ ë°°ê²½]: ì´ ì‚¬ê±´ì´ ì™œ ë°œìƒí–ˆëŠ”ì§€, ì´ì „ ìƒí™©ì€ ì–´ë– í–ˆëŠ”ì§€ ì„¤ëª… (3ë¬¸ì¥)\n"
                    "3. [ì „ë§ ë° ë¶„ì„]: ì•ìœ¼ë¡œì˜ ì˜í–¥ê³¼ í–¥í›„ ê´€ì „ í¬ì¸íŠ¸ ì œì‹œ (3ë¬¸ì¥)\n\n"
                    "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê³ ìœ ëª…ì‚¬ë¥¼ í¬í•¨í•˜ì—¬ ë¶„ì„ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
                )
            },
            {"role": "user", "content": f"ì œëª©: {title}\n\në³¸ë¬¸: {input_text[:3500]}"}
        ],
        "temperature": 0.5,
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=45)
        return r.json()['choices'][0]['message']['content'].strip()
    except:
        return None

# =========================
# 3. ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤
# =========================
def main():
    ensure_dir()
    collected_items = []
    seen_titles = set()

    for s in SOURCES:
        print(f"ğŸ“¡ ìˆ˜ì§‘ ì¤‘: {s['name']}...")
        feed = feedparser.parse(s["url"])
        count = 0
        for e in feed.entries:
            if count >= s["limit"]: break
            title = e.get("title", "").strip()
            link = e.get("link", "").strip()
            
            title_key = title[:12].replace(" ", "")
            if title_key in seen_titles: continue
            seen_titles.add(title_key)

            full_text = fetch_full_content(link) or clean_text(e.get("summary", ""))
            summary = openai_summary(title, full_text)
            
            collected_items.append({
                "category": s["id"],
                "source": s["name"],
                "title": title,
                "url": link,
                "published_at": datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M"),
                "summary": summary or "ìš”ì•½ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            })
            count += 1
            time.sleep(0.5)

    final_data = {
        "generated_at": datetime.now(tz=KST).isoformat(),
        "categories": DISPLAY_CATEGORIES,
        "items": collected_items[:65]
    }

    today = datetime.now(tz=KST).strftime("%Y-%m-%d")
    for filename in ["latest.json", f"{today}.json"]:
        with open(os.path.join(POSTS_DIR, filename), "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: ì´ {len(final_data['items'])}ê±´")

if __name__ == "__main__":
    main()
