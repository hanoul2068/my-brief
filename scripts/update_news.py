import os
import re
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from dateutil import tz
import time

# =========================
# 1. ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜
# =========================
KST = tz.gettz("Asia/Seoul")
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
POSTS_DIR = os.path.join(ROOT, "posts")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL") or "gpt-4o-mini"

# ë¶„ì•¼ë³„ ì •í™•ë„ê°€ ë†’ì€ RSSë¡œ ì¬ë°°ì • (ì´ 60~70ê°œ ìˆ˜ì§‘ -> ì¤‘ë³µ ì œê±° í›„ ì•½ 50ê±´ ìœ ì§€)
SOURCES = [
    {"id": "headline", "name": "ì£¼ìš”ë‰´ìŠ¤ (ì—°í•©TV)", "url": "http://www.yonhapnewstv.co.kr/browse/feed/", "limit": 10},
    {"id": "society", "name": "ì‚¬íšŒ (ì—°í•©ë‰´ìŠ¤)", "url": "https://www.yonhapnewsproxy.com/rss/society.xml", "limit": 12}, # ì£¼ì†Œ êµì²´
    {"id": "politics", "name": "ì •ì¹˜ (SBS)", "url": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER", "limit": 10},
    {"id": "economy", "name": "ê²½ì œ (í•œê²½)", "url": "https://www.hankyung.com/feed/economy", "limit": 10},
    {"id": "science", "name": "ê³¼í•™/ê¸°ìˆ  (YTN)", "url": "https://science.ytn.co.kr/ytnscience_rss.php", "limit": 10}, # ê³¼í•™ ì „ë¬¸ ì±„ë„
    {"id": "science", "name": "IT/í…Œí¬ (ë¸”ë¡œí„°)", "url": "https://www.bloter.net/rss/allNews.xml", "limit": 8}, # IT ì „ë¬¸ì§€
    {"id": "policy", "name": "ì •ì±…ë¸Œë¦¬í•‘", "url": "https://www.korea.kr/rss/policy.xml", "limit": 12},
]

DISPLAY_CATEGORIES = [
    {"id": "all", "name": "ì „ì²´"},
    {"id": "headline", "name": "ğŸ”¥ ì£¼ìš”ì†Œì‹"},
    {"id": "politics", "name": "âš–ï¸ ì •ì¹˜"},
    {"id": "economy", "name": "ğŸ’° ê²½ì œ/IT"},
    {"id": "society", "name": "ğŸ‘¥ ì‚¬íšŒ/ìƒí™œ"},
    {"id": "science", "name": "ğŸ§ª ê³¼í•™/ê¸°ìˆ "},
    {"id": "policy", "name": "ğŸ“¢ ì •ë¶€/ì •ì±…"}
]

# =========================
# 2. ìœ í‹¸ë¦¬í‹° ë° í¬ë¡¤ë§
# =========================
def ensure_dir():
    os.makedirs(POSTS_DIR, exist_ok=True)

def normalize_key(text: str, length: int = 15) -> str:
    text = re.sub(r'\[.*?\]|\(.*?\)', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.replace(" ", "")[:length]

def fetch_full_content(url: str) -> str:
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, "html.parser")
        for s in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'iframe']):
            s.decompose()
        # ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ ë³¸ë¬¸ ì˜ì—­ ëŒ€ì‘ ê°•í™”
        content = soup.find('article') or soup.find('div', id='articleBody') or soup.find('div', class_='article_view') or soup.find('div', id='news_body_area') or soup.find('div', class_='news_text')
        if content:
            text = content.get_text(" ", strip=True)
            return re.sub(r"\s+", " ", text).strip()
        return ""
    except:
        return ""

def openai_summary(title: str, content: str) -> str | None:
    if not OPENAI_API_KEY: return None
    input_text = content if len(content) > 150 else title
    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    
    payload = {
        "model": OPENAI_MODEL,
        "messages": [
            {
                "role": "system", 
                "content": (
                    "ë„ˆëŠ” ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ê°€ë‹¤. ë…ìê°€ ì›ë¬¸ì„ ë³´ì§€ ì•Šì•„ë„ ë§¥ë½ì„ ì™„ë²½íˆ ì´í•´í•˜ë„ë¡ ì‹¬ì¸µ ìš”ì•½ì„ ì œê³µí•œë‹¤. "
                    "ë‚´ìš©ì€ ë°˜ë“œì‹œ ë‹¤ìŒ 3ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´ 10ë¬¸ì¥ ë‚´ì™¸ë¡œ ì‘ì„±í•˜ë¼.\n\n"
                    "1. [í•µì‹¬ ì‚¬ì‹¤]: ì‚¬ê±´ì˜ í•µì‹¬ ìš”ì§€ë¥¼ ìƒì„¸íˆ ê¸°ìˆ  (3-4ë¬¸ì¥)\n"
                    "2. [ë§¥ë½ê³¼ ë°°ê²½]: ì´ ì‚¬ê±´ì´ ì™œ ë°œìƒí–ˆëŠ”ì§€, ì´ì „ ìƒí™©ì€ ì–´ë– í–ˆëŠ”ì§€ ì„¤ëª… (3ë¬¸ì¥)\n"
                    "3. [ì „ë§ ë° ë¶„ì„]: ì•ìœ¼ë¡œì˜ ì˜í–¥ê³¼ í–¥í›„ ê´€ì „ í¬ì¸íŠ¸ ì œì‹œ (3ë¬¸ì¥)\n\n"
                    "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ê³ ìœ ëª…ì‚¬ë¥¼ í¬í•¨í•˜ì—¬ ë¶„ì„ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
                )
            },
            {"role": "user", "content": f"ì œëª©: {title}\n\në³¸ë¬¸: {input_text[:3500]}"}
        ],
        "temperature": 0.5,
    }
    try:
        r = requests.post(url, headers=headers, json=payload, timeout=45)
        return r.json()['choices'][0]['message']['content'].strip()
    except:
        return None

# =========================
# 3. ë©”ì¸ í”„ë¡œì„¸ìŠ¤
# =========================
def main():
    ensure_dir()
    collected_items = []
    seen_keys = set()

    for s in SOURCES:
        print(f"ğŸ“¡ ìˆ˜ì§‘ ë° ê²€ì‚¬ ì¤‘: {s['name']}...")
        feed = feedparser.parse(s["url"])
        
        # RSS ì£¼ì†Œê°€ ì£½ì—ˆê±°ë‚˜ ì‘ë‹µì´ ì—†ëŠ” ê²½ìš° ì²´í¬
        if not feed.entries:
            print(f"âš ï¸ ê²½ê³ : {s['name']} í”¼ë“œê°€ ë¹„ì–´ìˆê±°ë‚˜ ì‘ë‹µì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        count = 0
        for e in feed.entries:
            if count >= s["limit"]: break
            
            title = e.get("title", "").strip()
            link = e.get("link", "").strip()
            
            # ì¤‘ë³µ ì²´í¬
            title_key = normalize_key(title, 15)
            if title_key in seen_keys: continue

            full_text = fetch_full_content(link) or title
            content_key = normalize_key(full_text, 30)
            if content_key and content_key in seen_keys: continue

            seen_keys.add(title_key)
            if content_key: seen_keys.add(content_key)

            summary = openai_summary(title, full_text)
            
            collected_items.append({
                "category": s["id"],
                "source": s["name"],
                "title": title,
                "url": link,
                "published_at": datetime.now(tz=KST).strftime("%Y-%m-%d %H:%M"),
                "summary": summary or "ì‹¬ì¸µ ë¶„ì„ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            })
            count += 1
            time.sleep(0.5)

    final_data = {
        "generated_at": datetime.now(tz=KST).isoformat(),
        "categories": DISPLAY_CATEGORIES,
        "items": collected_items[:65]
    }

    today = datetime.now(tz=KST).strftime("%Y-%m-%d")
    for filename in ["latest.json", f"{today}.json"]:
        with open(os.path.join(POSTS_DIR, filename), "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì™„ë£Œ: ì´ {len(final_data['items'])}ê±´ ì €ì¥.")

if __name__ == "__main__":
    main()
